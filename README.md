# streaming-examples
Streaming Cookbook

To create the quix.yaml file

```bash 
quix init
```

To create a local pipeline

```bash
quix local pipeline up
```

or

```bash
quix pipeline up
```

Running the above will create a compose.yaml file and spin up containers to run kafka locally and RedPanda(a webui to view kakfa messages)

Now, we can create producers / subscribers / demo data sources boilerplate with quix

```bash
quix local apps create
```

or

```bash
quix apps create
```

Running this  will give us a list of options to create various boilerplate/template items, select the demo data source and provide a name for the project. Here its demo-data. The folder includes a yaml file, demo-data.csv containing F1 Car Demo data, dockerfile, main.py and a requirements.txt file

Now install the required libraries through pip from the requirements.txt file via `pip install -r requirements.txt`

Navigate to the demo-data folder and download this sample data file

```bash
wget https://raw.githubusercontent.com/tomas-quix/streaming-academy/main/file-sink/demo_stream.json
```

The main.py contains additional code apart from the ones generated by the quix apps create. The default quix apps create creates the code to produce data to sent to the topic through a csv file. Custom code is return to support this for the json file. We will fetch data form the demo_stream.json file and send it to the kafka. Now we need to create a .env file. For this, we run

```bash
quix vars export
```

This will create a .env file with Quix_Broker_Address which is set to the Local address port and the output topic to which to send the streaming data

(`quix contexts broker set`, can be also used to set the local broker address)

Now within the demo-data run 

```bash
python main.py
```

This will start the producer and you can check out the redpanda ui at localhost:8080 to see the incoming stream of data

Now, we will call the below command
```bash
quix pipeline update
``` 
to update the microservice (demo-data producer) to the quix.yaml file. So after running the command, the quix.yaml file will be updated with a `deployments` key and the `name` under the deployments will be the name of the folder of the data source, i.e. the producer i.e. `demo-data`. Here `demo-data` is the demo data source which is publishing hard-coded lines of F1 telemetry JSON data to a Kafka topic, here the topic name is `f1-data`, which can be seen under the `variables` key under `deployments` in the quix.yaml file

Our `compose.yaml` will also get updated with a new `service` with the name as `demo-data`

Now after updating the quix pipeline, we run the below 
```bash
quix pipeline up
```
This will now create a new container for the `demo-data` service that is updated in the `compose.yaml` file. The dockerfile for this service will have been already created when we ran the `quix apps create` command. 

When the command has successfully run, we can observe the output with the below command

```bash
docker compose ps --format "table {{.Name}}\t{{.Status}}"
```

When we run this command we see 3 containers running. One is `streaming-examples-console-1` which sets up the redpanda, other is the `streaming-examples-kafka-broker-1` which sets up the kafka broker and we see a new container `streaming-examples-demo-data-1`, which we have just added to our compose.yaml and quix.yaml with the `quix pipeline update`. This container will publish F1 data line by line from a JSON file to the kafka topic. To see the logs we can run the below command

```bash
docker compose logs demo-data -f
```

We can now view the logs. The -f is for flowing logs, that is we can view the logs continously as they come up. This demo-data container will shutdown once it has published all the data from the demo_stream.json data

To view the pipeline, we can run the `quix pipeline view` which will create an image in the pipeline.md file. Now we are done with the data source part and will move on to the transformation / stream processing part

Now, we will run the `quix apps create` again, but this time to create a "Starter Transformation", which is responsible for consuming data from a topic, apply a simple transformation and then publish the transformed result to a new output topic. We name this folder `data-transform`

Now, we cd into the new micro service `data-transform` and open the `app.yaml` file. Here we check the value for the key `defaultValue` for the InputTopic under `variables`. We know that the `demo-data` we have created, will push the f1 telemery data to the topic named `f1-topic`. So, we need to fetch data from this topic to apply transformations to it. So we will change the value of the key `defaultValue` for the InputTopic to `f1-data`. Also, we change the `defaultValue` for the OutputTopic to `table-data` just so it looks good

Now, we run the `quix vars export` to create local environment variables. The env variables will contain the Quix Address Port, the Input and Output Topic. Then we write some transformation in the `main.py` file

Now, we again do the `quix pipeline update` to update the pipeline. This time, the quix.yaml, under the `deployments`, we will have two services. Even the pipeline.md will be updated with two services. Then we do the `quix pipeline up`, a new pipeline will be built in the `compose.yaml` file, under the `services`, we will get a new service `data-transform`. A new container will be built for this service

Now we run the 

```bash
docker compose ps --format "table {{.Name}}\t{{.Status}}"
```

We can see there will be 4 services. The newest container being the `streaming-examples-data-transform-1`. We can check the logs to view its output. We can open the RedPanda on the localhost:8080 and under the topics, we will find a new topic with the name `table-data` and it contains the messages with the transformations that we have applied with the data-transform service

## Stateless processing

- No context of previous messages. When we are processing a message, we do not care about the previous message
- No need to sticl messages to one partition/instance of microservice in consumer group
- We processes messages one at a time

Use Cases


- Schema Conversions (when we want to update how each message is represented / change how it is shown)
- Row/Column filtering (when we want to add / update / filter(remove) any columns or rows. For this we check each message one at a time and do not care about the previous messages)
- Feature computations inline (creating new features only using other columns values inside one row. Like we did in creating the acceleration-total feature, where we need the informatio from the current message and not relying on previous messages)

## Stateful processing

- Context of previous messages is needed as we may rely on previous messages
- Important to handle streams to avoid state conflicts
- state recovery

Use Cases:

- Aggregations - Sum, Max, Mean, Min of a variable per series
	- Eg: We are getting temperature stream for each second and want to get maximum temperature per day
- Aggregations per window of time
	- DownSampling, Data Normation, Context features
	- Eg: We are getting temperature stream for each second and we want the average temperature per hour. So we need all the previous messages

# Initial Code for data-tarnsform was this

```python
import os
from quixstreams import Application
import uuid
import json

# for local dev, load env vars from a .env file
from dotenv import load_dotenv

load_dotenv()


app = Application(consumer_group="data-transformation-v1", auto_offset_reset="earliest")

input_topic = app.topic(os.environ["input"])
output_topic = app.topic(os.environ["output"])

sdf = app.dataframe(input_topic)

sdf = sdf.apply(lambda msg: msg["payload"], expand=True)

# let us do some more transformations


def transform(row: dict) -> dict:
    new_row = {}
    new_row["time"] = row["time"]

    for key in row["values"]:
        new_row[row["name"] + "-" + key] = row["values"][key]

    return new_row


sdf = sdf.apply(transform)

sdf = sdf[sdf.contains("accelerometer-x")]

# create a new column
sdf["accelerometer-total"] = (
    sdf["accelerometer-x"].abs()
    + sdf["accelerometer-y"].abs()
    + sdf["accelerometer-z"].abs()
)

# update() function:
# Apply a function to mutate value in-place or to perform a side effect that doesn't update the value (e.g. print a value to the console).
sdf = sdf.update(lambda row: print(list(row.values())))

# now we publish the transformed data to a topic
sdf = sdf.to_topic(output_topic)

if __name__ == "__main__":
    app.run(sdf)

```

Now, we will update this data-transform file to do some state processing. What we did so far is stateless operation. We have the messages from the input topic, then for each message, we transformed the message by taking only the necessary columns and even creating a new column. So, the thing here is, we are dealing with individual messages and when we are working with a message, we are not concered about the previous messages, hence this is a stateless process



