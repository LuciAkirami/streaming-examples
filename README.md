# streaming-examples
Streaming Cookbook

To create the quix.yaml file

```bash 
quix init
```

To create a local pipeline

```bash
quix local pipeline up
```

or

```bash
quix pipeline up
```

Running the above will create a compose.yaml file and spin up containers to run kafka locally and RedPanda(a webui to view kakfa messages)

Now, we can create producers / subscribers / demo data sources boilerplate with quix

```bash
quix local apps create
```

or

```bash
quix apps create
```

Running this  will give us a list of options to create various boilerplate/template items, select the demo data source and provide a name for the project. Here its demo-data. The folder includes a yaml file, demo-data.csv containing F1 Car Demo data, dockerfile, main.py and a requirements.txt file

Now install the required libraries through pip from the requirements.txt file via `pip install -r requirements.txt`

Navigate to the demo-data folder and download this sample data file

```bash
wget https://raw.githubusercontent.com/tomas-quix/streaming-academy/main/file-sink/demo_stream.json
```

The main.py contains additional code apart from the ones generated by the quix apps create. The default quix apps create creates the code to produce data to sent to the topic through a csv file. Custom code is return to support this for the json file. We will fetch data form the demo_stream.json file and send it to the kafka. Now we need to create a .env file. For this, we run

```bash
quix vars export
```

This will create a .env file with Quix_Broker_Address which is set to the Local address port and the output topic to which to send the streaming data

(`quix contexts broker set`, can be also used to set the local broker address)

Now within the demo-data run 

```bash
python main.py
```

This will start the producer and you can check out the redpanda ui at localhost:8080 to see the incoming stream of data

Now, we will call the below command
```bash
quix pipeline update
``` 
to update the microservice (demo-data producer) to the quix.yaml file. So after running the command, the quix.yaml file will be updated with a `deployments` key and the `name` under the deployments will be the name of the folder of the data source, i.e. the producer i.e. `demo-data`. Here `demo-data` is the demo data source which is publishing hard-coded lines of F1 telemetry JSON data to a Kafka topic, here the topic name is `f1-data`, which can be seen under the `variables` key under `deployments` in the quix.yaml file

Our `compose.yaml` will also get updated with a new `service` with the name as `demo-data`

Now after updating the quix pipeline, we run the below 
```bash
quix pipeline up
```
This will now create a new container for the `demo-data` service that is updated in the `compose.yaml` file. The dockerfile for this service will have been already created when we ran the `quix apps create` command. 

When the command has successfully run, we can observe the output with the below command

```bash
docker compose ps --format "table {{.Name}}\t{{.Status}}"
```

When we run this command we see 3 containers running. One is `streaming-examples-console-1` which sets up the redpanda, other is the `streaming-examples-kafka-broker-1` which sets up the kafka broker and we see a new container `streaming-examples-demo-data-1`, which we have just added to our compose.yaml and quix.yaml with the `quix pipeline update`. This container will publish F1 data line by line from a JSON file to the kafka topic. To see the logs we can run the below command

```bash
docker compose logs demo-data -f
```

We can now view the logs. The -f is for flowing logs, that is we can view the logs continously as they come up. This demo-data container will shutdown once it has published all the data from the demo_stream.json data

To view the pipeline, we can run the `quix pipeline view` which will create an image in the pipeline.md file. Now we are done with the data source part and will move on to the transformation / stream processing part

Now, we will run the `quix apps create` again, but this time to create a "Starter Transformation", which is responsible for consuming data from a topic, apply a simple transformation and then publish the transformed result to a new output topic. We name this folder `data-transform`

Now, we cd into the new micro service `data-transform` and open the `app.yaml` file. Here we check the value for the key `defaultValue` for the InputTopic under `variables`. We know that the `demo-data` we have created, will push the f1 telemery data to the topic named `f1-topic`. So, we need to fetch data from this topic to apply transformations to it. So we will change the value of the key `defaultValue` for the InputTopic to `f1-data`. Also, we change the `defaultValue` for the OutputTopic to `table-data` just so it looks good

Now, we run the `quix vars export` to create local environment variables. The env variables will contain the Quix Address Port, the Input and Output Topic. Then we write some transformation in the `main.py` file

Now, we again do the `quix pipeline update` to update the pipeline. This time, the quix.yaml, under the `deployments`, we will have two services. Even the pipeline.md will be updated with two services. Then we do the `quix pipeline up`, a new pipeline will be built in the `compose.yaml` file, under the `services`, we will get a new service `data-transform`. A new container will be built for this service

Now we run the 

```bash
docker compose ps --format "table {{.Name}}\t{{.Status}}"
```

We can see there will be 4 services. The newest container being the `streaming-examples-data-transform-1`. We can check the logs to view its output. We can open the RedPanda on the localhost:8080 and under the topics, we will find a new topic with the name `table-data` and it contains the messages with the transformations that we have applied with the data-transform service

## Stateless processing

- No context of previous messages. When we are processing a message, we do not care about the previous message
- No need to sticl messages to one partition/instance of microservice in consumer group
- We processes messages one at a time

Use Cases


- Schema Conversions (when we want to update how each message is represented / change how it is shown)
- Row/Column filtering (when we want to add / update / filter(remove) any columns or rows. For this we check each message one at a time and do not care about the previous messages)
- Feature computations inline (creating new features only using other columns values inside one row. Like we did in creating the acceleration-total feature, where we need the informatio from the current message and not relying on previous messages)

## Stateful processing

- Context of previous messages is needed as we may rely on previous messages
- Important to handle streams to avoid state conflicts
- state recovery

Use Cases:

- Aggregations - Sum, Max, Mean, Min of a variable per series
	- Eg: We are getting temperature stream for each second and want to get maximum temperature per day
- Aggregations per window of time
	- DownSampling, Data Normation, Context features
	- Eg: We are getting temperature stream for each second and we want the average temperature per hour. So we need all the previous messages

# Initial Code for data-tarnsform was this

```python
import os
from quixstreams import Application
import uuid
import json

# for local dev, load env vars from a .env file
from dotenv import load_dotenv

load_dotenv()


app = Application(consumer_group="data-transformation-v1", auto_offset_reset="earliest")

input_topic = app.topic(os.environ["input"])
output_topic = app.topic(os.environ["output"])

sdf = app.dataframe(input_topic)

sdf = sdf.apply(lambda msg: msg["payload"], expand=True)

# let us do some more transformations


def transform(row: dict) -> dict:
    new_row = {}
    new_row["time"] = row["time"]

    for key in row["values"]:
        new_row[row["name"] + "-" + key] = row["values"][key]

    return new_row


sdf = sdf.apply(transform)

sdf = sdf[sdf.contains("accelerometer-x")]

# create a new column
sdf["accelerometer-total"] = (
    sdf["accelerometer-x"].abs()
    + sdf["accelerometer-y"].abs()
    + sdf["accelerometer-z"].abs()
)

# update() function:
# Apply a function to mutate value in-place or to perform a side effect that doesn't update the value (e.g. print a value to the console).
sdf = sdf.update(lambda row: print(list(row.values())))

# now we publish the transformed data to a topic
sdf = sdf.to_topic(output_topic)

if __name__ == "__main__":
    app.run(sdf)

```

Now, we will update this data-transform file to do some state processing. What we did so far is stateless operation. We have the messages from the input topic, then for each message, we transformed the message by taking only the necessary columns and even creating a new column. So, the thing here is, we are dealing with individual messages and when we are working with a message, we are not concered about the previous messages, hence this is a stateless process

So now, lets assume we are getting the data from a car and as the data comes, we need to update the odometer of the car. Let's assume the total distance of the car traveled is stored in the accelerometer-total column. So one way to do this is, create a odometer variable, then add the accelerometer-total value to the odometer and return it to an output topic. So let's try to implement this in the main.py of data-transform

```python
# we define an empty odo var
odometer = 0

# then we write a function to add the accelerometer-total to odo
# and we return the odo
def odo_calc(row):
    global odometer
    odometer += row["accelerometer-total"]
    return odometer

# then we apply this func to our sdf and the new stream will be the stream of odo values
sdf = sdf.apply(odo_calc)

# we console log the output for each message
sdf = sdf.update(lambda row: print(row))
```

Running this code will produce the below output (make sure that the containers and kakfa container are running. If not do `docker compose up`)
```
3371.240866929002
3371.496215024251
3371.870252042766
3372.038962888047
3372.1303216135784
3372.382108509214
3372.5501138365744
3372.6810873641357
3372.777936503777
```
It seems perfect. The odometer is increasing with each step. (if we run the demo-data main.py again, we can see that the odometer will start increasing again). Now let's say for some reason, this microservice gets crashed. Let's manually crash it ctrl-c and run the main.py of data-transform again. The below are the results

```
0.09351031407471745
0.1495566109454259
0.20797080599125473
0.3062925389356911
0.43033666909243906
0.5083161634929477
0.5739533320622517
0.7183588438548146
0.7947133308693766
0.8875286849498748
```

We see that the odometer starts again from zero. It does not begin from its previous value. This is a huge issue. This is because, the value is being stored in-memory of that service. So when service goes down, that value goes down and beings afresh from 0 when restarted. Now to solve this we use something called `State` from `quix-streams`

```python
from quixstreams import Application, State

def odo_func(row: dict, state: State):  # the order must be row, state and not reverse
    odometer = state.get(
        "odo", 0
    )  # get the odometer value is "odo" exists, else set the value to 0
    odometer += row["accelerometer-total"]
    state.set("odo", odometer)  # set the state with new odometer value

    return odometer


sdf = sdf.apply(
    odo_func, stateful=True
)  # we need to specify that its a statefull operation

sdf = sdf.update(lambda row: print(row))
```
Here, we import a `State` object from `quix-streams`. In this state object, we store our odometer. So we assign our key as "odo" in the state. And we will get it with `.get` and if it does not exist, then we will return 0. Then after adding the odometer, we will update the state by calling the `.set()` method and passing it the key and the new value. We need to make sure that when we are passing this function to the `.apply()` function, we need to set the `stateful=True`, which will let `quix` know that we are doing a stateful operation
