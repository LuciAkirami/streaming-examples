# streaming-examples
Streaming Cookbook

To create the quix.yaml file

```bash 
quix init
```

To create a local pipeline

```bash
quix local pipeline up
```

or

```bash
quix pipeline up
```

Running the above will create a compose.yaml file and spin up containers to run kafka locally and RedPanda(a webui to view kakfa messages)

Now, we can create producers / subscribers / demo data sources boilerplate with quix

```bash
quix local apps create
```

or

```bash
quix apps create
```

Running this  will give us a list of options to create various boilerplate/template items, select the demo data source and provide a name for the project. Here its demo-data. The folder includes a yaml file, demo-data.csv containing F1 Car Demo data, dockerfile, main.py and a requirements.txt file

Now install the required libraries through pip from the requirements.txt file via `pip install -r requirements.txt`

Navigate to the demo-data folder and download this sample data file

```bash
wget https://raw.githubusercontent.com/tomas-quix/streaming-academy/main/file-sink/demo_stream.json
```

The main.py contains additional code apart from the ones generated by the quix apps create. The default quix apps create creates the code to produce data to sent to the topic through a csv file. Custom code is return to support this for the json file. We will fetch data form the demo_stream.json file and send it to the kafka. Now we need to create a .env file. For this, we run

```bash
quix vars export
```

This will create a .env file with Quix_Broker_Address which is set to the Local address port and the output topic to which to send the streaming data

(`quix contexts broker set`, can be also used to set the local broker address)

Now within the demo-data run 

```bash
python main.py
```

This will start the producer and you can check out the redpanda ui at localhost:8080 to see the incoming stream of data

Now, we will call the below command
```bash
quix pipeline update
``` 
to update the microservice (demo-data producer) to the quix.yaml file. So after running the command, the quix.yaml file will be updated with a `deployments` key and the `name` under the deployments will be the name of the folder of the data source, i.e. the producer i.e. `demo-data`. Here `demo-data` is the demo data source which is publishing hard-coded lines of F1 telemetry JSON data to a Kafka topic, here the topic name is `f1-data`, which can be seen under the `variables` key under `deployments` in the quix.yaml file

Our `compose.yaml` will also get updated with a new `service` with the name as `demo-data`

Now after updating the quix pipeline, we run the below 
```bash
quix pipeline up
```
This will now create a new container for the `demo-data` service that is updated in the `compose.yaml` file. The dockerfile for this service will have been already created when we ran the `quix apps create` command. 

When the command has successfully run, we can observe the output with the below command

```bash
docker compose ps --format "table {{.Name}}\t{{.Status}}"
```

When we run this command we see 3 containers running. One is `streaming-examples-console-1` which sets up the redpanda, other is the `streaming-examples-kafka-broker-1` which sets up the kafka broker and we see a new container `streaming-examples-demo-data-1`, which we have just added to our compose.yaml and quix.yaml with the `quix pipeline update`. This container will publish F1 data line by line from a JSON file to the kafka topic. To see the logs we can run the below command

```bash
docker compose logs demo-data -f
```

We can now view the logs. The -f is for flowing logs, that is we can view the logs continously as they come up. This demo-data container will shutdown once it has published all the data from the demo_stream.json data

To view the pipeline, we can run the `quix pipeline view` which will create an image in the pipeline.md file. Now we are done with the data source part and will move on to the transformation / stream processing part

Now, we will run the `quix apps create` again, but this time to create a "Starter Transformation", which is responsible for consuming data from a topic, apply a simple transformation and then publish the transformed result to a new output topic. We name this folder `data-transform`