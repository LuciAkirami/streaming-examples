# streaming-examples
Streaming Cookbook

To create the quix.yaml file

```bash 
quix init
```

To create a local pipeline

```bash
quix local pipeline up
```

or

```bash
quix pipeline up
```

Running the above will create a compose.yaml file and spin up containers to run kafka locally and RedPanda(a webui to view kakfa messages)

Now, we can create producers / subscribers / demo data sources boilerplate with quix

```bash
quix local apps create
```

or

```bash
quix apps create
```

Running this  will give us a list of options to create various boilerplate/template items, select the demo data source and provide a name for the project. Here its demo-data. The folder includes a yaml file, demo-data.csv containing F1 Car Demo data, dockerfile, main.py and a requirements.txt file

Now install the required libraries through pip from the requirements.txt file via `pip install -r requirements.txt`

Navigate to the demo-data folder and download this sample data file

```bash
wget https://raw.githubusercontent.com/tomas-quix/streaming-academy/main/file-sink/demo_stream.json
```

The main.py contains additional code apart from the ones generated by the quix apps create. The default quix apps create creates the code to produce data to sent to the topic through a csv file. Custom code is return to support this for the json file. We will fetch data form the demo_stream.json file and send it to the kafka. Now we need to create a .env file. For this, we run

```bash
quix vars export
```

This will create a .env file with Quix_Broker_Address which is set to the Local address port and the output topic to which to send the streaming data

(`quix contexts broker set`, can be also used to set the local broker address)

Now within the demo-data run 

```bash
python main.py
```

This will start the producer and you can check out the redpanda ui at localhost:8080 to see the incoming stream of data